																
																DATA SCIENCE CONCEPTS

#INDEX:

1.  PYTHON
2.  VISUALIZING DATA
3.  LINEAR ALGEBRA
4.  STATISTICS
5.  PROBABILITY
6.  HYPOTHESIS AND INFERENCE
7.  GRADIENT DESCENT
8.  GETTING DATA
9.  WORKING WITH DATA
10. MACHINE LEARNING
11. k-NEAREST NEIGHBORS
12. NAIVE BAYES 
13. SIMPLE LINEAR REGRESSION
14. MULTIPLE REGRESSION
15. LOGISTIC REGRESSION
16. DECISSION TREES
17. NEURAL NETWORKS
18. DEEP LEARNING
19. CLUSTERING
20. NATURAL LANGUAGE PROCESSING
21. NETWORK ANALYSIS
22. DATABASES AND SQL



*********************************************************
*		*********************			*
*		*     1. PYTHON	    *			*
*		*********************			*
*********************************************************

1. VIRTUAL ENVIRONMENTS -> Anaconda Python Distribution 

	- Create virtual environment-> conda create -n namenv python=3.6

	- Activate -> source activate namenv
	- Deactivate -> source deactivate 

2. WHITESPACE FORMATTING -> Python uses indentation

	- Backlash (\)-> line continuation

3. MODULES (import, import module as)

4. FUNCTIONS -> takes 0 or more inputs and return the output:

	- def double(x):
		return x*2
		
	- We can assign them to variables and pass them into functions: 
	
		def apply_to_one(f):
			return f(1)
			
		my_double = double
		x= apply_to_one(my_double)
		
	- Anonimous functions (lambdas): y = apply_to_one(lambda x: x + 4)
	
5. STRINGS -> Double or single quotes 
	
	- tab -> "\t"
	- raw string -> r"\t" (represents \ and t)
	- Multiline strings: """ (three double quotes)
	
6. LISTS [] -> ordered collection (like an array), mutable, time consuming, homogeneous/heterogeneous: 

	- list = [1,2,3] 
	- list = ["string",0.1,True]
	- zero element -> list[0]
	- last element -> list[-1]
	- slice: i:j -> (inclusive, not inclusive)'; first_three = x[:3] ; one_to_four= x[1:5]
	  slice can take a third argument -> the stride: every_third = x[::3]

7. TUPLES ()-> immutable lists

	- tuple = (1,2)
	
8. DICTIONARIES {} -> associate values with keys

	- users =[{"id":0, "name":"Hero"}, 
			  {"id":1, "name":"Dunn"}, 		
						        ...
			 ]
			 
	- Lower case -> .lower()
	- Split -> .split()

	- defaultdict -> like a regular dictionary, except that when you try to look up a key it doesn’t contain,
					 it first adds a value for it using a zero-argument function you provided
					 when you created it. In order to use defaultdicts, you have to import them
					 from collections: 
					 
					 from collections import defaultdict
					 
9. COUNTERS -> turns a sequence of values into a defaultdict(int)-like object mapping keys to counts (from collections import Counter)

10. SETS {}-> represents a collection of distinct elements: primes_below_10 = {2, 3, 5, 7}
			  To create a empty set -> s = set() (s = {} will create a empty dict)
			  
	- in is very fast operation on sets
	- find distinct items in a collection
			  
11. CONTROL FLOW -> if, elif, else, while, for, in , continue, break

12. TRUTHINESS -> Booleans are capitalized (True or False)

13. SORTING -> Every python list has a .sort() method | sorted() method returns a new list

14. LIST COMPREHENSIONS -> transform a list into another one by choosing certain elements:

15. AUTOMATED TESTING AND ASSERT -> assert 1 + 1 == 2 , "should be equal to 2"
	
16. OBJECT-ORIENTED PROGRAMMING -> Define classes that contains zero or more member functions. By convention -> 1st parameter self and constructr __init__

	class MyClass:
		def __init__(self, count = 0) #class for counting
			self.count=count
			
	Count1 = MyClass() 				  # initialized to 0
	Count2 = MyClass(10)			  # starts with count = 10

	- __init__ with double underscores -> "magic" methods or "dunder" methods (double-underscore) and represent "special" behaviours, by convention -> private
	
	- Subclasses -> class (MyClass) -> inherit some of functionality of parent class
		
17. ITERABLES AND GENERATORS

def generate_range(n):
	i = 0
	while i < n:
		yield i # every call to yield produces a value of the generator
		i += 1

evens_below_20 = (i for i in generate_range(20) if i % 2 == 0) # generators by using for comprehensions wrapped in parentheses
		
	- Data pipelines:
	
	# None of these computations *does* anything until we iterate
	data = natural_numbers()
	evens = (x for x in data if x % 2 == 0)
	even_squares = (x ** 2 for x in evens)
	even_squares_ending_in_six = (x for x in even_squares if x % 10 == 6)
	# and so on
	
	- Enumerate functions: 
	
	- names = ["Alice", "Bob", "Charlie", "Debbie"]

	- for i, name in enumerate(names):
		print(f"name {i} is {name}")


18. RANDOMNESS -> 
	
	import random 					# pseudorandom
	random.seed(5) 
	four_uniform_randoms = [random.random() for _ in range(4)]
	
	- randrange(5) 					# choose randomly in [0,5)
	- randrange(a,b) 				# choose randomly in [a,b)
	- random.shuffle(list) 
	- random.choice(list) 			# pick one element, if calls several times chooses elements with replacement
	- random.sample(list) 			# choose elements without replacement
	
19. REGULAR EXPRESSIONS -> import re

	- re.match()  		# starts with? ; returns True or False
	- re.search()		# searches in
	- re.split()		# splits words
	- re.sub()			# replace

20. FUNCTIONAL PROGRAMMING
	
	zip & Argument unpacking:
	
		- zip: transforms multiple iterables into a single iterable of tuples of corresponding function
		
			[pair for pair in zip(list1, list2)]
		
		- argument unpacking: (*) asterisk performs argument unpacking:
		
			letters, numbers = zip(*pairs)  # uses the elements of pairs as individual arguments to zip
			
	args and kwargs: specify a function that takes arbitrary arguments
	
		def magic(*args, **kwargs):
			print("unnamed args:", args)
			print("keyword args:", kwargs)
			
	magic(1, 2, key="word", key2="word2")
	# prints
	# unnamed args: (1, 2)
	# keyword args: {'key': 'word', 'key2': 'word2'}	
	
		- args is a tuple of its unnamed arguments
		- kwargs is a dict of its named arguments
		
21. TYPE ANNOTATIONS -> Python is dynamically typed (not need to specify types)
			
		def add(a, b)						# without type annotations
		def add(a: int, b: int) -> int 	 	#with type annotations
			
		- inline type hints: from typing import Optional
		- first-class functions: from typing import Callable


*********************************************************
*		************************		*
*		* 2. VISUALIZING DATA  *		*
*		************************		*
*********************************************************

1. MATPLOTLIB -> python -m pip install matplotlib

		- from matplotlib impot pyplot as plt
		- plt.plot(data)
		- plt.title("Graph")
		- plt.ylabel("Y label")
		- plt.show()
			
2. BAR CHARTS -> plt.bar()
		
		- plt.xtickts   # label x-axis at bar centers
		
3. LINE CHARTS -> plt.plot() -> good choice for showing trends

4. SCATTER PLOTS -> plt.scatter(data1,data2)

- seaborn -> built on top of matplotlib for more complex visualizations
- Altair -> for declarative visualizations
- D3.js -> JavaScript library for interactive visualizations for the web
- Bokeh -> D3-style with Python			

*********************************************************
*	  *****************************			*
*	  *     3. LINEAR ALGEBRA     *			*
*	  *****************************			*
*********************************************************


1. VECTORS -> list of floats

	from typing import List
	Vector = List[float]

	- Build arithmetics since list's aren't vectors!
	
2. MATRICES -> list of lists -> Matrix = List[List[float]]

def make_matrix(num_rows: int, num_cols: int,entry_fn: Callable[[int, int], float]) -> Matrix:

"""
Returns a num_rows x num_cols matrix whose (i,j)-th entry is entry_fn(i, j)
"""
	return [[entry_fn(i, j) # given i, create a list
			  for j in range(num_cols)] # [entry_fn(i, 0), ... ]
			for i in range(num_rows)] # create one list for each i
			
def identity_matrix(n: int) -> Matrix:

"""Returns the n x n identity matrix"""

return make_matrix(n, n, lambda i, j: 1 if i == j else 0)

	- can represent a dataset consisting of multiple vectors by considering each vector as a row of the matrix
	- we can use an n × k matrix to represent a linear function that maps k-dimensional vectors to n-dimensional vectors
	- matrices can be used to represent binary relationships (create a matrix A such that A[i][j] is 1 if nodes i and j are connected and 0 otherwise)
	- Use NumPy!
	

*********************************************************
*	   *****************************		*
*	   *      4.  STATISTICS       *		*
*	   *****************************		*
*********************************************************

1. DESCRIBING A SINGLE DATASET -> from collections import Counter	
								  import matplotlib.pyplot as plt

	- len(data)
	- max(data)
	- min(data)
	
2. CENTRAL TENDENCIES:
	
	- mean -> average
	- median even, median odd, median -> middle-most value (if number of datapoints is odd) or average of two middle-most values (if number of datapoints is even)
	- quantile -> value under which a certain percentile of the data lies (median is 50% quantile)
	- mode -> most common value
		
	- Dispersion -> how spread the data is (0 -> not spread,)
	- Variance -> almost average squared deviation from the mean, how a single variable deviates from its mean
	- Standard deviation -> square root of the variance
	- Interquantile range -> difference between percentiles
	
3. CORRELATION

	- Covariance -> how two variables vary in tandem from their means
	- Correlation -> divides out the standard deviation of both variables; unitless between -1 (perfect anticorrelation) and 1(perfect correlation)

4. SYMPSON'S PARADOX -> Correlations can be misleading when confounding variables are ignored

-> SciPy , pandas, StatsModels


*********************************************************
*		*****************************		*
*		*     5.  PROBABILITY       *		*
*		*****************************		*
*********************************************************

	- P(E)-> probability of event E
	
1. DEPENDENCE AND INDEPENDENCE -> E and F are dependent if know if E happens gives info of F happening (and viceversa), otherwise they're independent.

	- P(E,F) = P(E)P(F)
	
2. CONDITIONAL PROBABILITY -> P(E|F)=P(E,F)/P(F) -> probability that E happens, given that we know F happens -> P(E,F)=P(E|F)P(E) 

	- If E and F are independent -> P(E|F)=P(E) -> knowing F ocurred gives no info about E ocurred
	
3. BAYES'S THEOREM -> "Reversing" conditional probabilities

	P(E|F)= P(F|E)P(E)/[P(F|E)P(E)+P(F|¬E)P(¬E)]
	
4. RANDOM VARIABLES -> associated to a probability distribution

5. CONTINUOS DISTRIBUTIONS 

	- Uniform distribution -> equal weight on all numbers between 0 and 1:
	
		- Density function 
		
			def uniform_pdf(x: float) -> float:
				return 1 if 0 <= x < 1 else 0
	
		- Cummulative distribution function (CDF) -> gives the probability that a random variable is less than or equal to a certain value
	
	- Normal distribution -> shaped by mean μ (where the bell is centered) and standard deviation σ (how wide is)
		
		- If μ = 0 and σ = 1 -> Standard normal distribution
		
6. THE CENTRAL LIMIT THEOREM -> random variable defined as the average of a large number of independent and identically distributed random variables 
								is itself approximately normally distributed.
	
	- Binomial random variables -> Binomial(n,p) -> sum of n independent Bernoulli(p) random variables, each of which equals 1 with probability p and 0 
													with probability 1-p
													
	- Bernoulli variable -> B(p) with mean p, and standard deviation SquareRoot(p(1-p)))	

	- If n gets large, a Binomial(n,p) variable is approximately a normal random variable with mean μ = np and standard deviation σ = SquareRoot(np(1-p))
	
- scipy.stats -> contains PDF and CDF of most popular


*************************************************
*	**********************************	*
*	*   6. HYPOTHESIS AND INFERENCE  *      *	
*	**********************************	*
*************************************************

1. STATISTICAL HYPOTHESIS TESTING -> H0 null hypothesis and H1 alternative hypothesis

	- Flipping a coin -> probability p of landing heads (p=0.5) tested against alternative hypothesis p ≠ 0.5.
						 Flipping the coin n times and count X heads. Each trial is a Bernoulli trial Binomial(n,p) random variable
	
	- p-Values -> Compute the probability  - assuming H0 is true- that we would see a value at least as extreme as the one we observed

		def two_sided_p_value(x: float, mu: float = 0, sigma: float = 1) -> float:
		"""
		How likely are we to see a value at least as extreme as x (in either
		direction) if our values are from an N(mu, sigma)?
			"""
			if x >= mu:
				# x is greater than the mean, so the tail is everything greater than x
				return 2 * normal_probability_above(x, mu, sigma)
			else:
				# x is less than the mean, so the tail is everything less than x
				return 2 * normal_probability_below(x, mu, sigma)
				
	- Confidence intervals -> confidence interval around the observed value of the parameter.
							  
	- p-Hacking -> consequence of the inference from p-values framework
				
	- Bayesian Inference -> treating the unknown parameters thenselves as random variables	
							Start with prior distribution + Observed data + Bayes's Theorem -> posterior distribution
							If the unknown parameter is a probability -> Beta distribution


*********************************************************
*		*****************************		*
*		*   7. GRADIENT DESCENT      *		*
*		*****************************		*
*********************************************************


1. THE IDEA BEHIND GRADIENT DESCENT 

	- gradient -> vector of partial derivatives, input direction the function most quickly increases

	- Pick a random starting point -> compute the gradient -> take small step in its direction -> repeat with new starting point

2. ESTIMATING THE GRADIENT

	def difference_quotient(f: Callable[[float], float],
							x: float,
							h: float) -> float:
	return (f(x + h) - f(x)) / h


	def estimate_gradient(f: Callable[[Vector], float],
						  v: Vector,
						  h: float = 0.0001):
	return [partial_difference_quotient(f, v, i, h)
			for i in range(len(v))]

3. CHOOSING THE RIGHT STEP SIZE

	- Using a fixed step size
	- Gradually shrinking the step size over time
	- At each step, choosing the step size that minimizes the value of the objective function

4. MINIBATCH AND STOCHASTIC GRADIENT DESCENT

	- Minibatch gradient descent -> compute the gradient based on a minibatch sampled from the larger dataset
	- Stochastic gradient descent -> take gradient steps based on one training example at a time
	


*********************************************************
*		*****************************		*
*		*    8. GETTING DATA         *		*
*		*****************************		*
*********************************************************									
									
	- sys.stdin & sys.stdout from the command line							
									
	- | pipe character -> use the output of the left as input for the right command	-> 
		-> cat SomeFile.txt | python egrep.py "[0-9]" | python line_count.py (Unix)
		-> type SomeFile.txt | python egrep.py "[0-9]" | python line_count.py (Windows)
									
	- Regular expresions -> import re								
	
	- Unix command line tools -> egrep, sed, cat, awk, grep
	
1. READING FILES

	# 'r' means read-only, it's assumed if you leave it out
	file_for_reading = open('reading_file.txt', 'r')
	file_for_reading2 = open('reading_file.txt')
	
	# 'w' is write -- will destroy the file if it already exists!
	file_for_writing = open('writing_file.txt', 'w')
	
	# 'a' is append -- for adding to the end of the file
	file_for_appending = open('appending_file.txt', 'a')

	# don't forget to close your files when you're done
	file_for_writing.close()
									
	- Use them within a with block ->	with open(filename) as f:							
	
	- Read a whole text file -> Iterate over the lines with for loop

	- .lower(); .split()
	
2. DELIMITED FILES

	- comma separated (CSV) -> import csv
	- tab separated -> \t -> csv.reader(f, delimiter='\t')

3. SCRAPPING THE WEB

	- HTML and the Parsing Thereof -> Beautiful Soup library -> python -m pip install beautifulsoup4 requests html5lib
	
	- Using APIs (application programming interfaces) -> request data in structured format
	
	- JSON (Javascript Object Notation) -> python json module (import json)
	
	- Pandas, Kaggle, Scrapy
	

*********************************************************
*		*****************************		*
*		*    9. WORKING WITH DATA   *		*
*		*****************************		*
*********************************************************

1. EXPLORING THE DATA

	- One dimensional -> create histograms
	
	- Two dimensional -> scatter the data
	
	- Many dimensions -> correlation matrix -> the entry in row i and column j is the correlation between the ith dimension 
											   and the jth dimension of the data
						
						-> Scatter plot matrix (when not many dimensions)

2. USING NAMEDTUPLES -> NamedTuples class (from collections import namedtuple) -> like a tuple but with named slots (unmutable)

3. DATACLASSES -> 	mutable version of NamedTuple -> from dataclasses import dataclass	-> use decorator (@dataclass)
									
4. CLEANING AND MUNGING								
									
	- Manipulating data
	
	- Rescaling -> each dimension has mean 0 nd std deviation 1
									
5. TQDM -> library for custom progress bars

6. DIMENSIONALITY REDUCTION

	PCA -> principal component analysis -> extract one or more dimensions that capture as much of the variation in data							
									
	- pandas, scikit-learn -> matrix decomposition functions (PCA included)


*********************************************************
*		*****************************		*
*		*    10. MACHINE LEARNING   *		*
*		*****************************		*
*********************************************************
	
1. MODELING

	- Creating models that are learned from data (predictive modeling, data mining)
	
		- Supervised models -> set of data labeled with the correct answers to learn from
	
		- Unsupervised models -> no labels
	
		- Semisupervised models -> some data labeled
	
		- Online -> model needs to adjust to newly arriving data
	
		- Reinforcement -> after making predictions, the model gets a signal indicating how well it did
	
	
2. OVERFITTING AND UNDERFITTING

	- Overfitting -> model performs well on training data but poorly to any new data
	
	- Underfitting -> model doesn't perform well even on training data
	
	- Most fundamental approach involves using different data to train the model and to test the model
	
3. CORRECTNESS

	- Accuracy -> fraction of correct predictions

	- Precision -> measures how accurate our positive predictions were
	
	- Recall -> measures what fraction of the positives our model identified
	
	- F1 score -> harmonic mean of precision and recall and lies between them
	
			def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:
				p = precision(tp, fp, fn, tn)
				r = recall(tp, fp, fn, tn)
				
				return 2 * p * r / (p + r)
	
4. THE BIAS-VARIANCE TRADEOFF -> another way of thinking of overfitting

	- high bias & low variance -> underfitting
	
	- low bias & high variance -> overfitting

	- Measures what happen if model is trained with different datasets
	

5. FEATURE EXTRACTION AND SELECTION

	- when data doesn't have enough features -> model likely to underfit

	- when data has too many features -> model likely to overfit

	- Features -> whatever inputs we provide to our model

		- Naive Bayes classifier
		
		- Regression models
	
		- Decision trees -> numerical and categorical data
									
									
*********************************************************
*		******************************		*
*		*    11. k-NEAREST NEIGHBORS *		*
*		******************************		*
*********************************************************								
									
1. THE MODEL

	- No math assumptions
		
	- Notion of distance
	
	- Points close are similar
									
	- Datapoints -> Vectors					
	
	- Reduce k until find a winner	
									
	- Example -> Iris dataset								
							
2.	THE COURSE OF DIMENSIONALITY -> k-Nearest neighbors algorithm runs into problem in higher dimensions -> vast -> no points closer to another

	- scikit-learn has nearest neighbors models.						
									
									
*********************************************************
*		*****************************		*
*		*    12. NAIVE BAYES        *		*
*		*****************************		*
*********************************************************									
									
	- Bayes Theorem -> P(E|F)= P(F|E)P(E)/[P(F|E)P(E)+P(F|¬E)P(¬E)]							
									
	- The key to Naive Bayes is making the (big) assumption that the presences (or absences)
	  of each word are independent of one another, conditional on a message being spam or not							
	
    - we’ll choose a pseudocount—k—and estimate the probability of seeing the ith word in a spam message as:

	P(Xi|S) = (k + number of spams containing wi)/(2k + number of spams)									
									
	- Using model -> SpamAssasin public corpus	

	- glob.glob() -> returns every filename that matches the wildcarded path									
									
	- scikit-learn -> BernoulliNB model 								
									
									
*********************************************************
*	***********************************		*
*	*    13. SIMPLE LINEAR REGRESSION *		*
*	***********************************		*
*********************************************************										
									
									
	- Correlation function -> Nature of relationship between variables?

1. THE MODEL -> yi = βxi + α + εi(error)

	- Coefficient of determination (R-squared) -> fraction of the total variation in the dependent variable that is captured by the model
									
	- If theta = [alpha,betha] -> using gradient descent		

	- Maximum Likelihood Estimation
									
	L(α,β|xi,yi,σ) = [1/SQRT(2πσ)]exp( − yi − α − βxi)^2/2σ^2								
									
									
*********************************************************
*		******************************		*
*		*   14. MULTIPLE REGRESSION  *          *		
*		******************************		*
*********************************************************										
									
1. THE MODEL

	- input is a vector of k numbers xi1,...xik	-> yi = α + β1xi1 + . . . + βkxik + εi

	- Vector of parameters -> β
	
	- Further assumptions
		
		- columns of x are linearly independent (no way to write any one as a weighted sum of some of the others)
		
		- columns of x are all uncorrelated the the errors ε
		
	- Fitting the model -> Choose β to minimize the sum of squarred errors -> use gradient descent
									
2. GOODNESS OF FIT

	- Adding new variables to a regression increaes the R-squared
	
	- assumption -> the errors εi are independent normal random variables with mean 0 and some shared
					(unknown) standard deviation σ
									
									
3. STANDARD ERRORS OF REGRESSION COEFFICIENTS

	- repeatedly take a bootstrap_sample of our data and estimate beta based on that sample	
									
	- Under the null hypothesis βi = 0 (and with our other assumptions about the distribution of εi), the statistic
	 
	   t_j = β_j/σ_j
									
	   which is our estimate of βj divided by our estimate of its standard error, follows a Student’s t-distribution with “n − k degrees of freedom.”								
									
4. REGULARIZATION 

	- add to the error term a penalty that gets larger as beta gets larger						
		
	- then minimize the combined error + penalty
	
	- Ridge regression -> penalty proportional to the sum of the squares of the beta_i (except beta_0, constant term)
	
	- Lasso regresion 
	
		def lasso_penalty(beta, alpha):
			return alpha * sum(abs(beta_i) for beta_i in beta[1:])
			
- scikit-learn -> linear_model module
	
- statsmodel -> python module
	
	
*********************************************************
*		******************************		*
*		*    15. LOGISTIC REGRESSION * 		*
*		******************************		*
*********************************************************
	
1. THE PROBLEM 

	- the outputs of the linear model can be huge positive numbers or even negative numbers
	
	- The linear regression model assumed that the errors were uncorrelated with the columns of x.
	
2. THE LOGISTIC FUNCTION

	def logistic(x: float) -> float:
		return 1.0 / (1 + math.exp(-x))
		
	We’ll use this to fit a model:
	
		yi = f(xiβ) + εi

	where f is the logistic function.
	
3. SUPPORT VECTOR MACHINES -> finds the hyperplane that maximizes the distane to the nearest point in each class

 - scikit-learn -> logistic regression, support vector machine -> LIBSUM

	
*********************************************************
*		*****************************		*
*		*    16. DECISION TREES     *		*
*		*****************************		*
*********************************************************	
	
1. WHAT IS A DECISION TREE -> tree structure to represent a number of possible decision paths and an outcome for each path
	
	- Classification trees -> categorical outputs
	
	- Regression trees -> produce numerical outputs
	
2. ENTROPY-> H(S) = -p1log2(p1) -...-pnlog2(pn) 
	
3. THE ENTROPY OF A PARTITION

	def partition_entropy(subsets: List[List[Any]]) -> float:
		"""Returns the entropy from this partition of data into subsets"""
		total_count = sum(len(subset) for subset in subsets)

		return sum(data_entropy(subset) * len(subset) / total_count
					for subset in subsets)
	
	- low entropy -> splits the data into subsets that themselves have low entropy
	
	- high entropy -> subsets that are large and have high entropy (highly uncertain)
	
4. CREATING A DECISSION TREE 
	
	- decision nodes -> ask a question and direct depending on answer
	
	- leaf nodes -> gives predictions
	
	- ID3 algorithm (greedy algorithm)-
	
5. RANDOM FOREST

	- Decission trees tend to overfit -> Random Forest -> build multiple DT and combine their outputs
	
	- scikit-learn -> decision trees , ensemble module
	- SGBoost -> library for training gradient boosted decision trees
	
		
*********************************************************
*		*****************************		*
*		*    17. NEURAL NETWORKS    *		*
*		*****************************		*
*********************************************************										
									
	- Artificial neural network -> predictive model motivated by the way the brain operates

1. PERCEPTRON -> single neuron with n binary inputs. Computes a weighted sum of its inputs and fires if that weighted sum is 0 or greater

	from scratch.linear_algebra import Vector, dot
	
	def step_function(x: float) -> float:
		return 1.0 if x >= 0 else 0.0

	def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:
	"""Returns 1 if the perceptron 'fires', 0 if not"""
	calculation = dot(weights, x) + bias
	return step_function(calculation)


	- AND/OR/NOT gates 
	
	- XOR gate cannot be build with perceptron

2. FEED-FORWARD NEURAL NETWORKS

	- approximate with an idealized feed-forward neural network of discrete layers of neurons connected to the next
	
3. BACKPROPAGATION -> use data to train neural network -> uses gradient descent or one of its variants


*********************************************************
*		*****************************		*
*		*    18. DEEP LEARNING      *		*
*		*****************************		*
*********************************************************

	- Deep neural networks -> more than one hidden layer 

1. THE TENSOR -> n-dimensional arrays -> list
	
2. THE LAYER ABSTRACTION 

	- Layer -> knows how to apply some function to its input and backpropagate gradients

		from scratch.neural_networks import sigmoid
		
		class Sigmoid(Layer):
			def forward(self, input: Tensor) -> Tensor:
				"""
				Apply sigmoid to each element of the input tensor,
				and save the results to use in backpropagation.
				"""
				self.sigmoids = tensor_apply(sigmoid, input)
				return self.sigmoids

			def backward(self, gradient: Tensor) -> Tensor:
			return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,
						  self.sigmoids, 
						  gradient)

3. THE LINEAR LAYER

	- Represent the dot(weights, inputs) part of the neurons
	
	- 3 different schemes:
		
		- Choose each value from the random uniform distribution on [0,1] (random.random())
		
		- Choose each value randomly from a standard normal distribution
		
		- Use Xavier initialization, where each weight is initialized with a random draw from a normal distribution with mean 0
		  and variance 2/(num_inputs + num_outputs)
	
	
4. NEURAL NETWORKS AS A SEQUENCE OF LAYERS -> ways to combine multiple layes into one

	- Loss and Optimization -> new loss abstraction that encapsulates both the loss computation and the gradient computation
	
								class Loss:
									def loss(self, predicted: Tensor, actual: Tensor) -> float:
									"""How good are our predictions? (Larger numbers are worse.)"""
									raise NotImplementedError
									
									def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:
									"""How does the loss change as the predictions change?"""
									raise NotImplementedError
		
	- Optimizer abstraction of which gradient descent will be specific instance
	
	- Optimization that uses momentum -> avoiding overreact to each new gradient
	
	- Example -> XOR | tang (hyperbolic tangent)
	
5. SOFTMAXES AND CROSS-ENTROPY -> forget about Sigmoid layer nd use the softmax function -> converts a vector of real numbers to a vector of 
																							probabilities
	
	- once the network produces probabilities -> use the cross-entropy function (or negative log likelihood)
	
	- our network outputs are probabilities, the crossentropy loss represents the negative log likelihood of the observed data, which means
	that minimizing that loss is the same as maximizing the log likelihood (and hence the likelihood) of the training data.
	
6. DROPOUT -> neural networks are prone to overfit to their training data -> using dropout ->

	- At training time, we randomnly turn off each neuron (replace output with 0) with some fixed probability
	
	- At evaluation time, we don't dropout any neurons
	
	- Example -> MNIST dataset of handwritten digits
	
7. SAVING AND LOADING MODELS -> models take a long time to train -> use json module to serialize models weights to a file
	
	- PyTorch for deep learning
	
	
*********************************************************
*		*****************************		*
*		*    19. CLUSTERING         *		*
*		*****************************		*
*********************************************************	
	
	- Unsupervised learning algorithm -> working with unlabeled data
	
1. THE IDEA -> data forms clusters

2. THE MODEL -> each input will be a vector in d-dimensional space (list of numbers) -> identify clusters of similar inputs

	- simple method -> k-means -> number of clusters k is chosen in advance -> partition the inputs into sets S1,...,Sk

	- assigning n points to k clusters:
	
		1. Start with a set of k-means, which are points in d-dimensional space.
		2. Assign each point to the mean to which it is closest.
		3. If no point’s assignment has changed, stop and keep the clusters.
		4. If some point’s assignment has changed, recompute the means and return to step 2.
	
3. CHOOSING K -> plotting the sum of squared errors as a function of k and looking at where the graph 'bends'
	
4. BOTTOM-UP HIERARCHICAL CLUSTERING -> alternative approach -> grow clusters from the bottom up
	
	- scikit-learn _> sklearn.cluster module (including KMeans and Ward hierarchical clustering)
	- SciPy -> two models -> scipy.cluster.vq -> k-means & scipy.cluster.hierarchy -> hierarchical clustering
	
	
*********************************************************
*	**************************************		*
*	*    20. NATURAL LANGUAGE PROCESSING * 		*		
*	**************************************		*
*********************************************************
	
		- NLP -> Computational techniques involving language
		
1. WORD CLOUDS -> depict words at sizes proportional to their counts
	
2. N-GRAM LANGUAGE MODELS -> modelling language with n consecutive words

3. GRAMMARS -> rules for generating acceptable sentences
	
4. GIBBS SAMPLING -> generating samples from multidimensional distributions when we only know some of
					 the conditional distributions
					
					The way Gibbs sampling works is that we start with any (valid) values for x and y and
					then repeatedly alternate replacing x with a random value picked conditional on y
					and replacing y with a random value picked conditional on x. After a number of iterations,
					the resulting values of x and y will represent a sample from the unconditional
					joint distribution


5. TOPIC MODELING -> try to identify the topics that underlie those interests.

					- latent Dirichlet allocation (LDA) is commonly used to identify common topics in a set of documents

						- There is some fixed number K of topics.
						
						- There is a random variable that assigns each topic an associated probability distribution
						over words. You should think of this distribution as the probability of
						seeing word w given topic k.
						
						- There is another random variable that assigns each document a probability distribution
						over topics. You should think of this distribution as the mixture of topics
						in document d.
						
						- Each word in a document was generated by first randomly picking a topic (from
						the document’s distribution of topics) and then randomly picking a word (from
						the topic’s distribution of words).


6. WORD VECTORS -> representing words as low-dimensional vectors

	- These vectors can be compared, added together, fed into machine learning models

		1. Get a bunch of text.
		2. Create a dataset where the goal is to predict a word given nearby words (or alternatively, to predict nearby words given a word).
		3. Train a neural net to do well on this task.
		4. Take the internal states of the trained neural net as the word vectors.

7. RECURRENT NEURAL NETWORKS

	- Uses words vectors as input -> have a hidden state they maintain between inputs
	- Simple case -> each input is combined with the current hidden state to produce an output that is used as the new hidden state
	- This allows such networks to “remember” (in a sense) the inputs they’ve seen, and to build up to a final output that depends 
	  on all the inputs and their order

	- NLTK -> NLP library for Python
	- gensim -> topic modelling
	- spaCy -> Industrial Strength NLP in Python
	- AllenNLP


*********************************************************
*	**************************************		*
*	*    21. NETWORK ANALYSIS   	     *		*
*	**************************************		*
*********************************************************

	- Networks -> nodes of some type and the edges that join them

1. BETWEENNESS CENTRALITY

	- identifies people who frequently are on the shortest paths between pairs of other people. In particular, the betweenness
	  centrality of node i is computed by adding up, for every other pair of nodes j and k, the proportion of shortest paths between
	  node j and node k that pass through i.

	- closeness centrality 
	
	- farness -> sum of the lengths of shorter paths to each other

	- large networks -> eigenvector centrality
	
2. EIGENVECTOR CENTRALITY

	- Eigenvectors and Matrix multiplication
	
	- Centrality ->  The eigenvector centrality for each user is then the entry corresponding to that user in the eigenvector

	- Eigenvector centrality -> numbers, one per user, such that each user’s value is a constant multiple of the sum of his neighbors’ values

3. DIRECTED GRAPHS AND PAGERANK 

	- DG -> track endorsements (source, target) 
	- PR -> better metric, who endorses you? -> PageRank based on link endorsements
	
	- NetworkX -> Python library for network analysis
	- Gephi -> GUI-based network visualization tool
	
	
	
	

*********************************************************
*	**************************************		*
*	*    22. DATABASE AND SQL   	     *		*
*	**************************************		*
*********************************************************

	- databases -> systems designed for efficietly storing and querying data
	
	- relational databases -> PostgreSQL, MySQL, SQL Server -> store data in tables and and queried using SQL
	
1. CREATE TABLE AND INSERT

	- relational database -> collection of tables and relationships among them

	- In SQL -> CREATE TABLE users (
					user_id INT NOT NULL,
					name VARCHAR(200),
					num_friends INT);

	- INSERT ROWS -> INSERT INTO users (user_id, name, num_friends) VALUES (0, 'Hero', 0);

	- SQL is almost case and identation-style insensitive
	
	- Statements need to end with semicolons
	
	- Single quotes for strings

2. UPDATE 

	UPDATE users
	SET num_friends = 3
	WHERE user_id = 1;

3. DELETE

	- DELETE FROM users;

	- DELETE FROM users WHERE user_id = 1;

4. SELECT

	- SELECT * FROM users; -> get the entire contents
	
	- SELECT * FROM users LIMIT 2; -> get the first two rows
	
	- SELECT user_id FROM users; -> only get specific columns
	
	- SELECT user_id FROM users WHERE name = 'Dunn'; -> only get specific rows

	- You can use SELECT STATEMENTS to calculate fields -> SELECT LENGTH(name) AS name_length FROM users;

5. GROUP BY

	- groups together rows with identical values in specified columns and produces aggregate values like MIN and MAX and COUNT and SUM 

		SELECT LENGTH(name) as name_length,
		MIN(user_id) AS min_user_id,
		COUNT(*) AS num_users
		FROM users
		GROUP BY LENGTH(name);

	- HAVING -> similar to WHERE but filter is applied to aggregates

6. ORDER BY 

	SELECT * FROM users
	ORDER BY name
	LIMIT 2;

	- SQL ORDER BY lets you specify ASC (ascending) or DESC (descending)

7. JOIN 

	- Relational databases tables are often normalized -> minimize redundancy
	
	- JOIN combines rows in the left table with corresponding rows in the right table
	
	SELECT users.name
	FROM users
	JOIN user_interests
	ON users.user_id = user_interests.user_id
	WHERE user_interests.interest = 'SQL'

	- LEFT JOIN 

8. SUBQUERIES

	SELECT MIN(user_id) AS min_user_id FROM
	(SELECT user_id FROM user_interests WHERE interest = 'SQL') sql_interests;

9. INDEXES -> each table can have one or more indexes, which allow to quickly look up rows by key columns

10. QUERY OPTIMIZATION ->  filter-before-join is more efficient

11. NoSQL -> Non-relational databases -> don't represent data in tables - <MongoDB (JSON documents rather than rows)

		- Columns database that store data in dolumns instead of rows

	- Relational databases -> SQLite, MySQL, PostgreSQL
	
	- NoSQL -> MongoDB

									
									
